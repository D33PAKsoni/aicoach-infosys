{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc17c53",
   "metadata": {},
   "source": [
    "Data Set Link: \n",
    "RAF DB: https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset/versions/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cecde5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths provided by you\n",
    "PRETRAINED_PATH = '/kaggle/input/fer2013/pytorch/default/1/best_fer2013_backbone.pth'\n",
    "TRAIN_DIR = '/kaggle/input/raf-db-dataset/DATASET/train'\n",
    "TEST_DIR = '/kaggle/input/raf-db-dataset/DATASET/test'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4  # Standard LR for fine-tuning\n",
    "EPOCHS = 15           # RAF-DB converges reasonably fast\n",
    "\n",
    "# RAF-DB Mapping (Ref: User provided)\n",
    "# 1:Surprise, 2:Fear, 3:Disgust, 4:Happiness, 5:Sadness, 6:Anger, 7:Neutral\n",
    "# Note: ImageFolder will map folder \"1\" to index 0, \"2\" to index 1, etc.\n",
    "# We will just verify this mapping in the print statements.\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA AUGMENTATION (Crucial for \"Real World\" Data)\n",
    "# ==========================================\n",
    "# We use stronger augmentation here than FER2013 because RAF-DB is color and high-res\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Important for webcam variance\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD DATA\n",
    "# ==========================================\n",
    "print(\"Loading Datasets...\")\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(TEST_DIR, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Classes found: {train_dataset.classes}\")\n",
    "print(f\"Example: Folder '{train_dataset.classes[0]}' mapped to Index 0\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODEL SETUP (The Intelligent Transfer)\n",
    "# ==========================================\n",
    "def load_model_safely(pretrained_path):\n",
    "    print(f\"\\nInitializing ResNet18...\")\n",
    "    model = models.resnet18()\n",
    "    \n",
    "    # 1. Match the architecture of the SAVED model (FER2013 had 7 classes)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 7)\n",
    "    \n",
    "    # 2. Load the weights\n",
    "    if os.path.exists(pretrained_path):\n",
    "        print(f\"Loading weights from: {pretrained_path}\")\n",
    "        try:\n",
    "            state_dict = torch.load(pretrained_path, map_location=DEVICE)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\">> FER2013 weights loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"!! Error loading weights: {e}\")\n",
    "            print(\"!! Starting from scratch (ImageNet weights) instead.\")\n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        print(\"!! Pretrained path not found. Using ImageNet weights.\")\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "    # 3. RESET THE HEAD\n",
    "    # The saved 'fc' layer knows FER2013 mapping (0=Angry).\n",
    "    # We need to learn RAF-DB mapping (0=Surprise).\n",
    "    # We re-initialize this layer to random weights.\n",
    "    print(\"Resetting final classification layer for RAF-DB...\")\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 7) # New random weights for 7 RAF-DB classes\n",
    "    \n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = load_model_safely(PRETRAINED_PATH)\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAINING LOOP\n",
    "# ==========================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Using a slightly lower LR for the backbone, but we update ALL parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_one_epoch(epoch_index):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    acc = 100 * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    acc = 100 * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "# ==========================================\n",
    "# 6. EXECUTION\n",
    "# ==========================================\n",
    "best_acc = 0.0\n",
    "print(\"\\nStarting RAF-DB Training...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_acc = validate()\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS}:\")\n",
    "    print(f\"  Train -> Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Valid -> Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_rafdb_model.pth')\n",
    "        print(f\"  >>> New Best Model Saved (Acc: {best_acc:.2f}%)\")\n",
    "\n",
    "print(\"\\nTraining Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584aa5c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = 'best_rafdb_model.pth'\n",
    "\n",
    "# Define the standard transforms (Validation version - no augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# RAF-DB Raw Classes (Mapped by Folder Name 1-7)\n",
    "# Note: ImageFolder sorts folders 1-7, so Index 0 is Folder '1', etc.\n",
    "RAW_CLASSES = {\n",
    "    0: 'Surprise',\n",
    "    1: 'Fear',\n",
    "    2: 'Disgust',\n",
    "    3: 'Happiness',\n",
    "    4: 'Sadness',\n",
    "    5: 'Anger',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD THE MODEL\n",
    "# ==========================================\n",
    "def load_inference_model():\n",
    "    model = models.resnet18()\n",
    "    # We must match the structure (7 classes) to load weights\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 7)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {MODEL_PATH} not found. Please train the model first.\")\n",
    "        return None\n",
    "        \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval() # Set to evaluation mode (freezes BatchNorm/Dropout)\n",
    "    return model\n",
    "\n",
    "model = load_inference_model()\n",
    "\n",
    "# ==========================================\n",
    "# 3. THE LOGIC LAYER (7 Classes -> 4 Interview Labels)\n",
    "# ==========================================\n",
    "def get_interview_prediction(probs):\n",
    "    \"\"\"\n",
    "    Manually sums probabilities to create Interview Categories.\n",
    "    Input: Tensor of 7 probabilities [Surprise, Fear, Disgust, Happy, Sad, Anger, Neutral]\n",
    "    \"\"\"\n",
    "    p = probs.tolist()[0] # Convert to standard list\n",
    "    \n",
    "    # 1. Stressed = Fear(1) + Sadness(4) + Surprise(0)\n",
    "    # We include Surprise because in interviews, shock often looks like stress.\n",
    "    score_stressed = p[1] + p[4] + p[0]\n",
    "    \n",
    "    # 2. Unprofessional = Disgust(2) + Anger(5)\n",
    "    score_unprofessional = p[2] + p[5]\n",
    "    \n",
    "    # 3. Confident = Happiness(3)\n",
    "    score_confident = p[3]\n",
    "    \n",
    "    # 4. Composed = Neutral(6)\n",
    "    score_composed = p[6]\n",
    "    \n",
    "    scores = {\n",
    "        \"Stressed\": score_stressed,\n",
    "        \"Unprofessional\": score_unprofessional,\n",
    "        \"Confident\": score_confident,\n",
    "        \"Composed\": score_composed\n",
    "    }\n",
    "    \n",
    "    # Return the category with the highest score\n",
    "    best_category = max(scores, key=scores.get)\n",
    "    return best_category, scores\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREDICTION FUNCTION\n",
    "# ==========================================\n",
    "def predict_emotion(image_path):\n",
    "    if model is None: return\n",
    "    \n",
    "    try:\n",
    "        # Load and Preprocess\n",
    "        img_raw = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = test_transforms(img_raw).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Forward Pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            # Apply Softmax to get probabilities (0.0 to 1.0)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get Raw RAF-DB Result\n",
    "        raw_score, raw_idx = torch.max(probs, 1)\n",
    "        raw_emotion = RAW_CLASSES[raw_idx.item()]\n",
    "        \n",
    "        # Get Interview Logic Result\n",
    "        interview_cat, interview_scores = get_interview_prediction(probs)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # Plot Image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_raw)\n",
    "        plt.title(f\"Input Image\\nRaw: {raw_emotion} ({raw_score.item():.2f})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot Interview Scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cats = list(interview_scores.keys())\n",
    "        vals = list(interview_scores.values())\n",
    "        colors = ['orange', 'red', 'green', 'blue'] # Stressed, Unprof, Confident, Composed\n",
    "        \n",
    "        plt.bar(cats, vals, color=colors)\n",
    "        plt.title(f\"Interview Coach Diagnosis: {interview_cat}\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Detailed Scores:\")\n",
    "        for cat, score in interview_scores.items():\n",
    "            print(f\"  {cat}: {score*100:.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. TEST IT HERE\n",
    "# ==========================================\n",
    "# Replace this path with any image from the RAF-DB Test folder \n",
    "# OR upload your own photo to Kaggle and copy the path.\n",
    "# Example path: '/kaggle/input/raf-db-dataset/DATASET/Test/1/test_0001.jpg'\n",
    "\n",
    "test_image_path = '/kaggle/input/raf-db-dataset/DATASET/test/5/test_0031_aligned.jpg' # Update this!\n",
    "\n",
    "# Check if file exists before running to avoid error spam\n",
    "import os\n",
    "if os.path.exists(test_image_path):\n",
    "    predict_emotion(test_image_path)\n",
    "else:\n",
    "    print(f\"Please update 'test_image_path' to a valid file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca134584",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12947a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download the BlazeFace model if you haven't already\n",
    "MODEL_PATH = 'blaze_face_short_range.tflite'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"Downloading MediaPipe Face Detection model...\")\n",
    "    os.system(f\"wget -q -O {MODEL_PATH} https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e716f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# New MediaPipe Imports\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = 'best_rafdb_model.pth' # Make sure this matches your saved file name\n",
    "\n",
    "# Standard transforms\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "RAW_CLASSES = {0:'Surprise', 1:'Fear', 2:'Disgust', 3:'Happiness', \n",
    "               4:'Sadness', 5:'Anger', 6:'Neutral'}\n",
    "\n",
    "# ==========================================\n",
    "# 2. FACE CROPPER (New Tasks API Version)\n",
    "# ==========================================\n",
    "class FaceCropper:\n",
    "    def __init__(self, model_path='blaze_face_short_range.tflite'):\n",
    "        # Create FaceDetector options\n",
    "        base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "        options = vision.FaceDetectorOptions(base_options=base_options, min_detection_confidence=0.5)\n",
    "        self.detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "    def get_crop(self, image_path):\n",
    "        # Read image with OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None: return None\n",
    "        \n",
    "        # Convert to RGB (MediaPipe requires RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load into MediaPipe Image format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "\n",
    "        # Detect\n",
    "        detection_result = self.detector.detect(mp_image)\n",
    "        \n",
    "        if detection_result.detections:\n",
    "            # Get the first face\n",
    "            detection = detection_result.detections[0]\n",
    "            bbox = detection.bounding_box\n",
    "            \n",
    "            # The Tasks API gives absolute pixel coordinates directly\n",
    "            x = bbox.origin_x\n",
    "            y = bbox.origin_y\n",
    "            w = bbox.width\n",
    "            h = bbox.height\n",
    "            \n",
    "            ih, iw, _ = image.shape\n",
    "            \n",
    "            # --- Smart Margin Logic (10%) ---\n",
    "            margin_x = int(w * 0.1)\n",
    "            margin_y = int(h * 0.1)\n",
    "            \n",
    "            x = max(0, x - margin_x)\n",
    "            y = max(0, y - margin_y)\n",
    "            w = min(iw - x, w + 2 * margin_x)\n",
    "            h = min(ih - y, h + 2 * margin_y)\n",
    "            \n",
    "            # Crop\n",
    "            face_crop = image_rgb[y:y+h, x:x+w]\n",
    "            \n",
    "            # Fallback if crop is empty\n",
    "            if face_crop.size == 0:\n",
    "                return Image.fromarray(image_rgb)\n",
    "                \n",
    "            return Image.fromarray(face_crop)\n",
    "        \n",
    "        else:\n",
    "            print(\"Warning: No face detected. Using full image.\")\n",
    "            return Image.fromarray(image_rgb)\n",
    "\n",
    "# Initialize Cropper\n",
    "cropper = FaceCropper()\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD MODEL & LOGIC\n",
    "# ==========================================\n",
    "def load_inference_model():\n",
    "    model = models.resnet18()\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 7)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {MODEL_PATH} not found. Please train/upload the model.\")\n",
    "        return None\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval() \n",
    "    return model\n",
    "\n",
    "model = load_inference_model()\n",
    "\n",
    "def get_interview_prediction(probs):\n",
    "    p = probs.tolist()[0]\n",
    "    scores = {\n",
    "        \"Stressed\": p[1] + p[4] + p[0],       # Fear + Sad + Surprise\n",
    "        \"Unprofessional\": p[2] + p[5],        # Disgust + Anger\n",
    "        \"Confident\": p[3],                    # Happy\n",
    "        \"Composed\": p[6]                      # Neutral\n",
    "    }\n",
    "    return max(scores, key=scores.get), scores\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREDICT FUNCTION\n",
    "# ==========================================\n",
    "def predict_emotion(image_path):\n",
    "    if model is None: return\n",
    "\n",
    "    # A. Get Crop\n",
    "    face_img = cropper.get_crop(image_path)\n",
    "    if face_img is None:\n",
    "        print(\"Error: Could not read image file.\")\n",
    "        return\n",
    "\n",
    "    # B. Predict\n",
    "    img_tensor = test_transforms(face_img).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    raw_idx = torch.max(probs, 1)[1].item()\n",
    "    interview_cat, interview_scores = get_interview_prediction(probs)\n",
    "    \n",
    "    # C. Visualize\n",
    "    original_img = Image.open(image_path)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Original Input\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(face_img)\n",
    "    plt.title(f\"MediaPipe Crop\\nRaw: {RAW_CLASSES[raw_idx]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    cats = list(interview_scores.keys())\n",
    "    vals = list(interview_scores.values())\n",
    "    colors = ['orange', 'red', 'green', 'blue']\n",
    "    plt.bar(cats, vals, color=colors)\n",
    "    plt.title(f\"Result: {interview_cat}\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 5. TEST\n",
    "# ==========================================\n",
    "# Update this path\n",
    "test_image_path = '/kaggle/input/myphotos/WIN_20260131_14_49_36_Pro.jpg' \n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    predict_emotion(test_image_path)\n",
    "else:\n",
    "    print(f\"Please update 'test_image_path' to a valid file path.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
